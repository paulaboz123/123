{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure ML v2 — Batch Endpoint (ModelBatchDeployment)\n",
        "\n",
        "Ten notebook tworzy **\"lepszą\" prostszą wersję** batch endpointu, tj. **ModelBatchDeployment** (bez parallel/`amlbi_main.py`, bez wymogu `azureml-core`).\n",
        "\n",
        "Co robi:\n",
        "1) Łączy się z Azure ML workspace (`MLClient`)\n",
        "2) Tworzy **3 AmlCompute clustery** (PL/EN/DE) z `min_instances=0` → **brak kosztów gdy brak jobów**\n",
        "3) Tworzy **1 Batch Endpoint**\n",
        "4) Tworzy **3 ModelBatchDeployment** (PL/EN/DE), każdy przypięty do innego clustra\n",
        "5) (Opcjonalnie) Uruchamia 3 batch joby równolegle (po jednym na język)\n",
        "\n",
        "## Dlaczego to jest \"lepsze\" w Twoim case\n",
        "- Nie uruchamia się driver `driver/amlbi_main.py`\n",
        "- Nie potrzebujesz paczek `azureml-core` (SDK v1) w env\n",
        "- Izolujesz zasoby per język przez osobne clustery, nadal z `min=0`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Instalacja (jeśli potrzebujesz)\n",
        "Jeśli odpalasz lokalnie, odkomentuj."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -U azure-ai-ml azure-identity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Konfiguracja\n",
        "Uzupełnij dane workspace + nazwy model/env. W prod zalecane jest podawanie konkretnych wersji modeli i env."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SUBSCRIPTION_ID = \"<SUBSCRIPTION_ID>\"\n",
        "RESOURCE_GROUP  = \"<RESOURCE_GROUP>\"\n",
        "WORKSPACE_NAME  = \"<WORKSPACE_NAME>\"\n",
        "\n",
        "# Batch endpoint\n",
        "BATCH_ENDPOINT_NAME = \"doc-classifier-batch\"\n",
        "\n",
        "# Environment (zarejestrowany w AML)\n",
        "ENV_NAME    = \"model-x-env\"\n",
        "ENV_VERSION = \"5\"\n",
        "\n",
        "# 3 modele (zarejestrowane w AML)\n",
        "MODELS = {\n",
        "    \"pl\": {\"name\": \"model-x-pl\", \"version\": \"1\"},\n",
        "    \"en\": {\"name\": \"model-x-en\", \"version\": \"1\"},\n",
        "    \"de\": {\"name\": \"model-x-de\", \"version\": \"1\"},\n",
        "}\n",
        "\n",
        "# Code + scoring\n",
        "CODE_DIR = \"./src\"\n",
        "SCORING_SCRIPT = \"score.py\"  # batch: musi zawierać init() + run(mini_batch)\n",
        "\n",
        "# Compute\n",
        "COMPUTE_SIZE = \"Standard_DS3_v2\"  # dopasuj do potrzeb\n",
        "MIN_NODES = 0  # 0 -> brak kosztów gdy brak jobów\n",
        "MAX_NODES_PER_LANG = {\"pl\": 4, \"en\": 4, \"de\": 4}\n",
        "\n",
        "# Jak szybko wygaszać VM po zakończeniu jobów (sekundy)\n",
        "IDLE_TIME_BEFORE_SCALE_DOWN = 120\n",
        "\n",
        "# Równoległość per VM\n",
        "MAX_CONCURRENCY_PER_INSTANCE = 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) MLClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(),\n",
        "    subscription_id=SUBSCRIPTION_ID,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WORKSPACE_NAME,\n",
        ")\n",
        "\n",
        "print(\"Workspace:\", ml_client.workspaces.get(WORKSPACE_NAME).name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Utwórz 3 AmlCompute clustery (min=0)\n",
        "Osobny cluster per język = DE nie zabierze node'ów EN. Brak kosztu idle dzięki `min_instances=0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "compute_names = {lang: f\"cpu-batch-{lang}\" for lang in MODELS.keys()}\n",
        "\n",
        "for lang, cname in compute_names.items():\n",
        "    compute = AmlCompute(\n",
        "        name=cname,\n",
        "        size=COMPUTE_SIZE,\n",
        "        min_instances=MIN_NODES,\n",
        "        max_instances=MAX_NODES_PER_LANG[lang],\n",
        "        idle_time_before_scale_down=IDLE_TIME_BEFORE_SCALE_DOWN,\n",
        "    )\n",
        "    print(f\"Creating/updating compute: {cname} (min={MIN_NODES}, max={MAX_NODES_PER_LANG[lang]})\")\n",
        "    ml_client.compute.begin_create_or_update(compute).result()\n",
        "\n",
        "print(\"Compute ready:\", compute_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Pobierz Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = ml_client.environments.get(name=ENV_NAME, version=ENV_VERSION)\n",
        "print(\"Env:\", env.name, env.version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Utwórz 1 Batch Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import BatchEndpoint\n",
        "\n",
        "endpoint = BatchEndpoint(\n",
        "    name=BATCH_ENDPOINT_NAME,\n",
        "    description=\"ModelBatchDeployment batch endpoint for PL/EN/DE with separate compute clusters (min=0).\",\n",
        ")\n",
        "\n",
        "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
        "print(\"Batch endpoint ready:\", BATCH_ENDPOINT_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Utwórz 3 ModelBatchDeployment (to jest klucz)\n",
        "Ta ścieżka NIE używa parallel drivera (`amlbi_main.py`).\n",
        "\n",
        "Każdy deployment:\n",
        "- ma inny model (PL/EN/DE)\n",
        "- używa tego samego `score.py`\n",
        "- wskazuje inny compute cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import ModelBatchDeployment\n",
        "\n",
        "deployment_names = {lang: f\"deploy-{lang}\" for lang in MODELS.keys()}\n",
        "\n",
        "for lang, spec in MODELS.items():\n",
        "    model = ml_client.models.get(name=spec[\"name\"], version=spec[\"version\"])\n",
        "\n",
        "    dep = ModelBatchDeployment(\n",
        "        name=deployment_names[lang],\n",
        "        endpoint_name=BATCH_ENDPOINT_NAME,\n",
        "        model=model,\n",
        "        environment=env,\n",
        "        code_configuration={\"code\": CODE_DIR, \"scoring_script\": SCORING_SCRIPT},\n",
        "        compute=compute_names[lang],\n",
        "        instance_count=1,\n",
        "        max_concurrency_per_instance=MAX_CONCURRENCY_PER_INSTANCE,\n",
        "    )\n",
        "\n",
        "    print(f\"Creating/updating deployment {dep.name}: model={model.name}:{model.version}, compute={compute_names[lang]}\")\n",
        "    ml_client.batch_deployments.begin_create_or_update(dep).result()\n",
        "\n",
        "print(\"Deployments ready:\", deployment_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) (Opcjonalnie) Default deployment\n",
        "Jeśli czasem nie podajesz `deployment_name` przy jobie, ustaw default. Przy routing po języku zwykle i tak podajesz deployment_name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "endpoint = ml_client.batch_endpoints.get(BATCH_ENDPOINT_NAME)\n",
        "endpoint.default_deployment_name = deployment_names[\"en\"]\n",
        "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
        "print(\"Default deployment:\", endpoint.default_deployment_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) (Opcjonalnie) Uruchom 3 joby równolegle (PL+EN+DE)\n",
        "To odpali 3 niezależne batch joby. Dzięki osobnym clustrom nie będą walczyć o te same node'y.\n",
        "\n",
        "Ustaw ścieżki input/output w datastore.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import BatchJob, Input\n",
        "import time\n",
        "\n",
        "INPUTS = {\n",
        "    \"pl\": \"azureml://datastores/workspaceblobstore/paths/in/pl/\",\n",
        "    \"en\": \"azureml://datastores/workspaceblobstore/paths/in/en/\",\n",
        "    \"de\": \"azureml://datastores/workspaceblobstore/paths/in/de/\",\n",
        "}\n",
        "\n",
        "OUTPUTS = {\n",
        "    \"pl\": \"azureml://datastores/workspaceblobstore/paths/out/pl/\",\n",
        "    \"en\": \"azureml://datastores/workspaceblobstore/paths/out/en/\",\n",
        "    \"de\": \"azureml://datastores/workspaceblobstore/paths/out/de/\",\n",
        "}\n",
        "\n",
        "submitted = {}\n",
        "ts = int(time.time())\n",
        "\n",
        "for lang in MODELS.keys():\n",
        "    job = BatchJob(\n",
        "        name=f\"run-{lang}-{ts}\",\n",
        "        endpoint_name=BATCH_ENDPOINT_NAME,\n",
        "        deployment_name=deployment_names[lang],\n",
        "        inputs={\"input_data\": Input(type=\"uri_folder\", path=INPUTS[lang])},\n",
        "        outputs={\"output_data\": Input(type=\"uri_folder\", path=OUTPUTS[lang])},\n",
        "    )\n",
        "\n",
        "    created = ml_client.batch_jobs.begin_create_or_update(job).result()\n",
        "    submitted[lang] = created.name\n",
        "    print(\"Submitted:\", lang, created.name)\n",
        "\n",
        "print(\"All jobs submitted:\", submitted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Status jobów\n",
        "Po zakończeniu, node'y wygasną automatycznie po `idle_time_before_scale_down` (przy `min=0`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_job(job_name: str):\n",
        "    j = ml_client.batch_jobs.get(job_name)\n",
        "    print(f\"{job_name}: status={j.status}\")\n",
        "\n",
        "# for lang, job_name in submitted.items():\n",
        "#     show_job(job_name)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}