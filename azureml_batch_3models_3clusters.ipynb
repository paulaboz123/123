{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure ML v2 — Batch Endpoint: 3 modele (PL/EN/DE) na 1 endpoint + 3 compute clustery (min=0)\n",
        "\n",
        "Ten notebook robi:\n",
        "1. Logowanie do Azure ML (v2 SDK) i utworzenie `MLClient`\n",
        "2. Utworzenie **3 AmlCompute clusterów** (`min_instances=0`) — brak kosztów, gdy nie ma jobów\n",
        "3. Utworzenie (lub update) **jednego Batch Endpointu**\n",
        "4. Utworzenie (lub update) **3 Batch Deploymentów** (PL/EN/DE), każdy przypięty do osobnego clustra\n",
        "5. (Opcjonalnie) Uruchomienie 3 batch jobów równolegle (po jednym na język)\n",
        "\n",
        "## Ważne o kosztach i opóźnieniach\n",
        "- `min_instances=0` na clustrze oznacza: **0 VM działających bez jobów → 0 kosztu idle**.\n",
        "- Pierwszy job po okresie bezczynności ma **cold start** (provisioning node’ów). To jest nieuniknione przy `min=0`.\n",
        "- Żeby użytkownicy nie „czekali na siebie” przy równoległych językach, robimy **osobne clustery per język**.\n",
        "- Dodatkowo ustawiamy `idle_time_before_scale_down` na **niski** poziom, aby VM szybko się wyłączały po jobie.\n",
        "\n",
        "> Uwaga: Nie da się mieć jednocześnie: (a) zerowego kosztu idle i (b) absolutnie zerowego cold startu. Można tylko zbalansować.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Instalacja / wersje\n",
        "Upewnij się, że masz `azure-ai-ml` i `azure-identity`.\n",
        "Jeśli uruchamiasz lokalnie, odkomentuj komendę instalacji."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -U azure-ai-ml azure-identity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Konfiguracja: Workspace, nazwy zasobów, modele, env, ścieżki kodu\n",
        "Wypełnij pola poniżej."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- WORKSPACE ---\n",
        "SUBSCRIPTION_ID = \"<SUBSCRIPTION_ID>\"\n",
        "RESOURCE_GROUP  = \"<RESOURCE_GROUP>\"\n",
        "WORKSPACE_NAME  = \"<WORKSPACE_NAME>\"\n",
        "\n",
        "# --- BATCH ENDPOINT ---\n",
        "BATCH_ENDPOINT_NAME = \"doc-classifier-batch\"  # nazwa jednego batch endpointu\n",
        "\n",
        "# --- ENVIRONMENT (zarejestrowane w AML) ---\n",
        "ENV_NAME    = \"model-x-env\"\n",
        "ENV_VERSION = \"5\"\n",
        "\n",
        "# --- MODELE (zarejestrowane w AML) ---\n",
        "# Jeśli masz konkretne wersje - podaj je (zalecane w prod).\n",
        "MODELS = {\n",
        "    \"pl\": {\"name\": \"model-x-pl\", \"version\": \"1\"},\n",
        "    \"en\": {\"name\": \"model-x-en\", \"version\": \"1\"},\n",
        "    \"de\": {\"name\": \"model-x-de\", \"version\": \"1\"},\n",
        "}\n",
        "\n",
        "# --- CODE + SCORING SCRIPT ---\n",
        "# Folder z kodem (np. zawiera score.py oraz dodatkowe pliki)\n",
        "CODE_DIR = \"./src\"\n",
        "SCORING_SCRIPT = \"score.py\"\n",
        "\n",
        "# --- COMPUTE: 3 clustery (po jednym na język) ---\n",
        "# Size dobierz do potrzeb; to jest przykład CPU.\n",
        "COMPUTE_SIZE = \"Standard_DS3_v2\"\n",
        "\n",
        "# min_instances=0 -> brak kosztów bez jobów\n",
        "MIN_NODES = 0\n",
        "\n",
        "# max_instances -> limit równoległości (ile VM może maksymalnie wystartować na klastrze)\n",
        "# Ustaw tak, aby obsłużyć równoległe joby / wolumen dokumentów.\n",
        "MAX_NODES_PER_LANG = {\n",
        "    \"pl\": 4,\n",
        "    \"en\": 4,\n",
        "    \"de\": 4,\n",
        "}\n",
        "\n",
        "# Jak szybko klaster ma gasić VM po bezczynności (sekundy).\n",
        "# Niższa wartość = mniej kosztu po jobie, ale częstsze cold starty.\n",
        "IDLE_TIME_BEFORE_SCALE_DOWN = 120\n",
        "\n",
        "# Równoległość wewnątrz node’a (ile „porcji pracy” naraz na jednej VM)\n",
        "MAX_CONCURRENCY_PER_INSTANCE = 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Logowanie i MLClient\n",
        "Używamy `DefaultAzureCredential` (działa w Azure ML Compute Instance, VS Code + az login, Managed Identity itd.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(),\n",
        "    subscription_id=SUBSCRIPTION_ID,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WORKSPACE_NAME,\n",
        ")\n",
        "\n",
        "print(\"Connected to:\", ml_client.workspaces.get(WORKSPACE_NAME).name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Utworzenie 3 AmlCompute clusterów (min=0)\n",
        "Każdy język dostaje własny cluster, co zapewnia izolację capacity:\n",
        "- job DE nie może „zabrać” node’ów EN\n",
        "- oba mogą skalować się niezależnie do `max_nodes`\n",
        "\n",
        "Uwaga: cluster jako zasób może istnieć stale, ale **koszt generują tylko działające node’y**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "compute_names = {\n",
        "    lang: f\"cpu-batch-{lang}\" for lang in MODELS.keys()\n",
        "}\n",
        "\n",
        "for lang, cname in compute_names.items():\n",
        "    compute = AmlCompute(\n",
        "        name=cname,\n",
        "        size=COMPUTE_SIZE,\n",
        "        min_instances=MIN_NODES,                    # 0 -> brak kosztów, gdy nie ma jobów\n",
        "        max_instances=MAX_NODES_PER_LANG[lang],     # limit ile VM może wystartować\n",
        "        idle_time_before_scale_down=IDLE_TIME_BEFORE_SCALE_DOWN,  # szybkie gaszenie po jobie\n",
        "    )\n",
        "    print(f\"Creating/updating compute: {cname} (min={MIN_NODES}, max={MAX_NODES_PER_LANG[lang]})\")\n",
        "    ml_client.compute.begin_create_or_update(compute).result()\n",
        "\n",
        "print(\"Compute clusters ready:\", compute_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Pobranie Environment\n",
        "Deployment batch potrzebuje środowiska (conda/docker) zarejestrowanego w AML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = ml_client.environments.get(name=ENV_NAME, version=ENV_VERSION)\n",
        "print(\"Using env:\", env.name, env.version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Utworzenie (lub update) Batch Endpointu\n",
        "Jeśli endpoint już istnieje, kod go zaktualizuje (bez niszczenia deploymentów)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import BatchEndpoint\n",
        "\n",
        "endpoint = BatchEndpoint(\n",
        "    name=BATCH_ENDPOINT_NAME,\n",
        "    description=\"Batch endpoint for PL/EN/DE models (separate compute clusters, min=0).\",\n",
        ")\n",
        "\n",
        "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
        "print(\"Batch endpoint ready:\", BATCH_ENDPOINT_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Utworzenie 3 Batch Deploymentów (PL/EN/DE) na jednym endpoint\n",
        "Każdy deployment:\n",
        "- wskazuje inny model (PL/EN/DE)\n",
        "- ma to samo `score.py`\n",
        "- używa dedykowanego clustra (`compute=cpu-batch-<lang>`)\n",
        "\n",
        "Dzięki temu równoległe joby na różne języki nie blokują się nawzajem (blokada byłaby tylko na poziomie ich własnego clustra)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import ModelBatchDeployment\n",
        "\n",
        "deployment_names = {lang: f\"deploy-{lang}\" for lang in MODELS.keys()}\n",
        "\n",
        "for lang, spec in MODELS.items():\n",
        "    model = ml_client.models.get(name=spec[\"name\"], version=spec[\"version\"])\n",
        "    dep = ModelBatchDeployment(\n",
        "        name=deployment_names[lang],\n",
        "        endpoint_name=BATCH_ENDPOINT_NAME,\n",
        "        model=model,\n",
        "        environment=env,\n",
        "        code_configuration={\n",
        "            \"code\": CODE_DIR,           # folder z kodem\n",
        "            \"scoring_script\": SCORING_SCRIPT,  # entrypoint batch\n",
        "        },\n",
        "        compute=compute_names[lang],\n",
        "        instance_count=1,  # zwykle zostawia się 1; klaster skaluje node’y wg potrzeby\n",
        "        max_concurrency_per_instance=MAX_CONCURRENCY_PER_INSTANCE,  # równoległość per VM\n",
        "    )\n",
        "\n",
        "    print(f\"Creating/updating deployment: {dep.name} -> model={model.name}:{model.version}, compute={compute_names[lang]}\")\n",
        "    ml_client.batch_deployments.begin_create_or_update(dep).result()\n",
        "\n",
        "print(\"Deployments ready:\", deployment_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) (Opcjonalnie) Ustawienie default deployment\n",
        "Batch endpoint nie ma traffic split jak online. Możesz ustawić deployment domyślny, jeśli czasem nie podajesz `deployment_name` przy jobie.\n",
        "W praktyce przy routing po języku zwykle **zawsze podajesz deployment_name**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "endpoint = ml_client.batch_endpoints.get(BATCH_ENDPOINT_NAME)\n",
        "endpoint.default_deployment_name = deployment_names[\"en\"]  # np. EN jako default\n",
        "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
        "print(\"Default deployment set to:\", endpoint.default_deployment_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) (Opcjonalnie) Uruchomienie jobów równolegle: PL + EN + DE\n",
        "Poniżej przykład jak odpalić trzy niezależne batch joby w tym samym czasie.\n",
        "\n",
        "### Input/Output\n",
        "- Ustaw `INPUT_*` na swoje ścieżki w datastore.\n",
        "- `uri_folder` jest typowe przy dokumentach (PDF, itp.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import BatchJob, Input\n",
        "import time\n",
        "\n",
        "INPUTS = {\n",
        "    \"pl\": \"azureml://datastores/workspaceblobstore/paths/in/pl/\",\n",
        "    \"en\": \"azureml://datastores/workspaceblobstore/paths/in/en/\",\n",
        "    \"de\": \"azureml://datastores/workspaceblobstore/paths/in/de/\",\n",
        "}\n",
        "\n",
        "OUTPUTS = {\n",
        "    \"pl\": \"azureml://datastores/workspaceblobstore/paths/out/pl/\",\n",
        "    \"en\": \"azureml://datastores/workspaceblobstore/paths/out/en/\",\n",
        "    \"de\": \"azureml://datastores/workspaceblobstore/paths/out/de/\",\n",
        "}\n",
        "\n",
        "submitted = {}\n",
        "\n",
        "for lang in MODELS.keys():\n",
        "    job = BatchJob(\n",
        "        name=f\"run-{lang}-{int(time.time())}\",\n",
        "        endpoint_name=BATCH_ENDPOINT_NAME,\n",
        "        deployment_name=deployment_names[lang],  # routing po języku\n",
        "        inputs={\n",
        "            \"input_data\": Input(type=\"uri_folder\", path=INPUTS[lang])\n",
        "        },\n",
        "        outputs={\n",
        "            \"output_data\": Input(type=\"uri_folder\", path=OUTPUTS[lang])\n",
        "        },\n",
        "    )\n",
        "\n",
        "    created = ml_client.batch_jobs.begin_create_or_update(job).result()\n",
        "    submitted[lang] = created.name\n",
        "    print(\"Submitted:\", lang, created.name)\n",
        "\n",
        "print(\"All jobs submitted:\", submitted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Sprawdzenie statusu jobów\n",
        "Compute ma `min=0`, więc po zakończeniu jobów VM zostaną automatycznie wygaszone po `idle_time_before_scale_down`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_job(job_name: str):\n",
        "    j = ml_client.batch_jobs.get(job_name)\n",
        "    print(f\"{job_name}: status={j.status}, created={j.creation_context.created_at}\")\n",
        "\n",
        "# Podmień na swoje job_id lub użyj 'submitted' ze wcześniejszej komórki.\n",
        "# for lang, job_name in submitted.items():\n",
        "#     show_job(job_name)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}