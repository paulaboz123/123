{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure ML v2 — Batch Endpoint **BEZ parallel** (wymuszenie `type: model` przez YAML schema)\n",
        "\n",
        "Ten notebook **wymusza** tworzenie Batch Deployments jako **Model Batch Deployment** (bez `driver/amlbi_main.py`).\n",
        "\n",
        "Dlaczego YAML?\n",
        "- W części środowisk/wersji SDK, tworzenie deploymentu przez obiekt Pythona bywa mapowane do trybu parallel.\n",
        "- YAML schema `modelBatchDeployment.schema.json` z `type: model` jest najpewniejszym sposobem na **brak parallel runtime**.\n",
        "\n",
        "Co robi:\n",
        "1) Tworzy 3 clustery AmlCompute (`min_instances=0`) → **0 kosztu bez jobów**\n",
        "2) Tworzy 1 Batch Endpoint\n",
        "3) Generuje 3 pliki YAML deploymentów (`type: model`) dla PL/EN/DE\n",
        "4) Deployuje je do endpointu\n",
        "5) Ustawia default deployment (opcjonalnie)\n",
        "6) (Opcjonalnie) Uruchamia joby kodem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Instalacja (jeśli potrzebujesz)\n",
        "Odkomentuj, jeśli pracujesz lokalnie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install -U azure-ai-ml azure-identity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Konfiguracja\n",
        "Uzupełnij workspace, env i modele. W prod dawaj konkretne wersje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SUBSCRIPTION_ID = \"<SUBSCRIPTION_ID>\"\n",
        "RESOURCE_GROUP  = \"<RESOURCE_GROUP>\"\n",
        "WORKSPACE_NAME  = \"<WORKSPACE_NAME>\"\n",
        "\n",
        "BATCH_ENDPOINT_NAME = \"doc-classifier-batch\"\n",
        "\n",
        "ENV_NAME    = \"model-x-env\"\n",
        "ENV_VERSION = \"5\"\n",
        "\n",
        "MODELS = {\n",
        "    \"pl\": {\"name\": \"model-x-pl\", \"version\": \"1\"},\n",
        "    \"en\": {\"name\": \"model-x-en\", \"version\": \"1\"},\n",
        "    \"de\": {\"name\": \"model-x-de\", \"version\": \"1\"},\n",
        "}\n",
        "\n",
        "CODE_DIR = \"./src\"\n",
        "SCORING_SCRIPT = \"score.py\"  # musi mieć init() + run(mini_batch)\n",
        "\n",
        "# 3 osobne clustery, 0 kosztu bez jobów\n",
        "COMPUTE_SIZE = \"Standard_DS3_v2\"\n",
        "MIN_NODES = 0\n",
        "MAX_NODES_PER_LANG = {\"pl\": 4, \"en\": 4, \"de\": 4}\n",
        "IDLE_TIME_BEFORE_SCALE_DOWN = 120\n",
        "\n",
        "YAML_DIR = \"./deploy_yamls\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) MLClient + wersja SDK (debug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import azure.ai.ml\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "print(\"azure-ai-ml version:\", azure.ai.ml.__version__)\n",
        "\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(),\n",
        "    subscription_id=SUBSCRIPTION_ID,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WORKSPACE_NAME,\n",
        ")\n",
        "\n",
        "print(\"Workspace:\", ml_client.workspaces.get(WORKSPACE_NAME).name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Utwórz 3 AmlCompute clustery (min=0)\n",
        "Izolacja zasobów per język + 0 kosztu bez jobów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "compute_names = {lang: f\"cpu-batch-{lang}\" for lang in MODELS.keys()}\n",
        "\n",
        "for lang, cname in compute_names.items():\n",
        "    compute = AmlCompute(\n",
        "        name=cname,\n",
        "        size=COMPUTE_SIZE,\n",
        "        min_instances=MIN_NODES,\n",
        "        max_instances=MAX_NODES_PER_LANG[lang],\n",
        "        idle_time_before_scale_down=IDLE_TIME_BEFORE_SCALE_DOWN,\n",
        "    )\n",
        "    print(f\"Creating/updating compute: {cname} (min={MIN_NODES}, max={MAX_NODES_PER_LANG[lang]})\")\n",
        "    ml_client.compute.begin_create_or_update(compute).result()\n",
        "\n",
        "print(\"Compute ready:\", compute_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Utwórz 1 Batch Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import BatchEndpoint\n",
        "\n",
        "endpoint = BatchEndpoint(\n",
        "    name=BATCH_ENDPOINT_NAME,\n",
        "    description=\"Batch endpoint without parallel runtime (type:model deployments via YAML).\",\n",
        ")\n",
        "\n",
        "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
        "print(\"Batch endpoint ready:\", BATCH_ENDPOINT_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Wygeneruj YAML-e deploymentów `type: model` i je zdeployuj\n",
        "To jest **klucz**: schema `modelBatchDeployment` + `type: model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from azure.ai.ml import load_batch_deployment\n",
        "\n",
        "Path(YAML_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "deployment_names = {lang: f\"deploy-{lang}-model\" for lang in MODELS.keys()}\n",
        "schema_url = \"https://azuremlschemas.azureedge.net/latest/modelBatchDeployment.schema.json\"\n",
        "\n",
        "for lang, spec in MODELS.items():\n",
        "    dep_name = deployment_names[lang]\n",
        "    yml_path = Path(YAML_DIR) / f\"{dep_name}.yml\"\n",
        "\n",
        "    yml = f\"\"\"\\\n",
        "$schema: {schema_url}\n",
        "name: {dep_name}\n",
        "endpoint_name: {BATCH_ENDPOINT_NAME}\n",
        "type: model\n",
        "\n",
        "model: azureml:{spec['name']}:{spec['version']}\n",
        "environment: azureml:{ENV_NAME}:{ENV_VERSION}\n",
        "\n",
        "code_configuration:\n",
        "  code: {CODE_DIR}\n",
        "  scoring_script: {SCORING_SCRIPT}\n",
        "\n",
        "compute: azureml:{compute_names[lang]}\n",
        "\n",
        "resources:\n",
        "  instance_count: 1\n",
        "\"\"\"\n",
        "\n",
        "    yml_path.write_text(yml, encoding=\"utf-8\")\n",
        "    print(\"Wrote:\", str(yml_path))\n",
        "\n",
        "    dep = load_batch_deployment(str(yml_path))\n",
        "    print(f\"Deploying {dep_name} (type:model) -> compute={compute_names[lang]}\")\n",
        "    ml_client.batch_deployments.begin_create_or_update(dep).result()\n",
        "\n",
        "print(\"Deployments created:\", deployment_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Ustaw default deployment (opcjonalnie)\n",
        "UI często odpala job na default dep, jeśli nie wybierzesz innego."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ep = ml_client.batch_endpoints.get(BATCH_ENDPOINT_NAME)\n",
        "ep.default_deployment_name = deployment_names[\"en\"]\n",
        "ml_client.batch_endpoints.begin_create_or_update(ep).result()\n",
        "print(\"default_deployment_name =\", ep.default_deployment_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Debug: wypisz deployment i sprawdź, że to nie parallel\n",
        "Jeśli nadal widzisz `amlbi_main.py`, to znaczy, że Studio odpala job na innym (starym) deploymencie lub w trybie parallel z UI.\n",
        "Wtedy uruchom job **kodem** (pkt 8)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "deps = list(ml_client.batch_deployments.list(endpoint_name=BATCH_ENDPOINT_NAME))\n",
        "print([d.name for d in deps])\n",
        "\n",
        "d = ml_client.batch_deployments.get(endpoint_name=BATCH_ENDPOINT_NAME, name=deployment_names[\"en\"])\n",
        "print(\"DEP CLASS:\", type(d))\n",
        "print(d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) (Opcjonalnie) Uruchom job KODEM (pewniej niż UI)\n",
        "Podmień input/output (folder z plikami JSON, jeśli score.py czyta JSON)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import BatchJob, Input\n",
        "import time\n",
        "\n",
        "INPUTS = {\n",
        "    \"pl\": \"azureml://datastores/workspaceblobstore/paths/in/pl/\",\n",
        "    \"en\": \"azureml://datastores/workspaceblobstore/paths/in/en/\",\n",
        "    \"de\": \"azureml://datastores/workspaceblobstore/paths/in/de/\",\n",
        "}\n",
        "\n",
        "OUTPUTS = {\n",
        "    \"pl\": \"azureml://datastores/workspaceblobstore/paths/out/pl/\",\n",
        "    \"en\": \"azureml://datastores/workspaceblobstore/paths/out/en/\",\n",
        "    \"de\": \"azureml://datastores/workspaceblobstore/paths/out/de/\",\n",
        "}\n",
        "\n",
        "ts = int(time.time())\n",
        "submitted = {}\n",
        "\n",
        "for lang in MODELS.keys():\n",
        "    job = BatchJob(\n",
        "        name=f\"run-{lang}-{ts}\",\n",
        "        endpoint_name=BATCH_ENDPOINT_NAME,\n",
        "        deployment_name=deployment_names[lang],\n",
        "        inputs={\"input_data\": Input(type=\"uri_folder\", path=INPUTS[lang])},\n",
        "        outputs={\"output_data\": Input(type=\"uri_folder\", path=OUTPUTS[lang])},\n",
        "    )\n",
        "\n",
        "    created = ml_client.batch_jobs.begin_create_or_update(job).result()\n",
        "    submitted[lang] = created.name\n",
        "    print(\"Submitted:\", lang, created.name)\n",
        "\n",
        "print(\"Submitted jobs:\", submitted)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}